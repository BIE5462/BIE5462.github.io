<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>BP神经网络 | BIEBLOG</title>
  
  <meta name="keywords" content="数据结构与算法,智能计算,最优化理论">
  
  
  <meta name="description" content="我的学习之路">
  

  

  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">
  

  

  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/css/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  
  
  <div class="cover-wrapper">
    <cover class='cover post half'>
      
        
  <h1 class='title'>BIEBOLG</h1>


  <div class="m_search">
    <form name="searchform" class="form u-search-form">
      <input type="text" class="input u-search-input" placeholder="查找文章" />
      <i class="icon fas fa-search fa-fw"></i>
    </form>
  </div>

<div class='menu navgation'>
  <ul class='h-list'>
    
      
        <li>
          <a class="nav home" href="/"
            
            
            id="home">
            <i class='fas fa-rss fa-fw'></i>&nbsp;全部文章
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/category"
            
            
            id="category">
            <i class='fas fa-folder-open fa-fw'></i>&nbsp;文章分类
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/tag"
            
            
            id="tag">
            <i class='fas fa-hashtag fa-fw'></i>&nbsp;文章标签
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/about"
            
              rel="nofollow"
            
            
            id="about">
            <i class='fas fa-info-circle fa-fw'></i>&nbsp;关于本站
          </a>
        </li>
      
    
  </ul>
</div>

      
    </cover>
    <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>

	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          BIEBLOG
        
      </a>
			<div class='menu navgation'>
				<ul class='h-list'>
          
  					
  						<li>
								<a class="nav flat-box" href="/"
                  
                  
                  id="home">
									<i class='fas fa-grin fa-fw'></i>&nbsp;主页
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/category"
                  
                    rel="nofollow"
                  
                  
                  id="category">
									<i class='fas fa-folder-open fa-fw'></i>&nbsp;分类
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/tag"
                  
                    rel="nofollow"
                  
                  
                  id="tag">
									<i class='fas fa-hashtag fa-fw'></i>&nbsp;标签
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/timeline"
                  
                    rel="nofollow"
                  
                  
                  id="timeline">
									<i class='fas fa-history fa-fw'></i>&nbsp;时间轴
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
        
          <li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/"
                
                
                id="home">
								<i class='fas fa-rss fa-fw'></i>&nbsp;全部文章
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/category"
                
                
                id="category">
								<i class='fas fa-folder-open fa-fw'></i>&nbsp;文章分类
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/tag"
                
                
                id="tag">
								<i class='fas fa-hashtag fa-fw'></i>&nbsp;文章标签
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about"
                
                  rel="nofollow"
                
                
                id="about">
								<i class='fas fa-info-circle fa-fw'></i>&nbsp;关于本站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>

  </div>


  <div class="l_body">
    <div class='body-wrapper'>
      <div class='l_main'>
  

  
    <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
    
    <div class="meta" id="header-meta">
      
        
  
    <h1 class="title">
      <a href="/2019/08/24/20190824shenjing/">
        BP神经网络
      </a>
    </h1>
  


      
      <div class='new-meta-box'>
        
          
        
          
            
  <div class='new-meta-item author'>
    <a href="http://example.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>杨春波</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-08-24</p>
  </a>
</div>

          
        
          
            
  
  <div class='new-meta-item category'>
    <a href='/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/' rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>专业知识</p>
    </a>
  </div>


          
        
          
            
  
    <div class="new-meta-item browse busuanzi">
      <a class='notlink'>
        <i class="fas fa-eye" aria-hidden="true"></i>
        <p>
          <span id="busuanzi_value_page_pv">
            <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
          </span>
        </p>
      </a>
    </div>
  


          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          <p>神经网络算法能够通过大量的历史数据，逐步建立和完善输入变量到输出结果之间的发展路径，也就是神经网络，在这个神经网络中，每条神经的建立以及神经的粗细（权重）都是经过大量历史数据训练得到的，数据越多，神经网络就越接近真实。神经网络建立后，就能够通过不同的输入变量值，预测输出结果</p>
<span id="more"></span>
<h1 id="八月第三次汇报"><a href="#八月第三次汇报" class="headerlink" title="八月第三次汇报"></a>八月第三次汇报</h1><ul>
<li>本周继续学习JavaScript基础和相关模板使用</li>
<li>数学建模基础方法</li>
<li>以下整理数学建模人工神经网络的基础知识</li>
</ul>
<h1 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>现实生活中，很多事情人们往往只关注结果输出，而忽略输入因素，以及输入到输出的具体过程；即使很多有心人想要总结输入到输出的路径规律，也会发现涉及的输入影响因素（变量）太多，因素之间还可能存在交互作用，导致输入到输出的路径错综复杂，就像人的神经网络一样，很难发现从输入到输出这个过程的具体规律。</p>
<p>神经网络算法能够通过大量的历史数据，逐步建立和完善输入变量到输出结果之间的发展路径，也就是神经网络，在这个神经网络中，每条神经的建立以及神经的粗细（权重）都是经过大量历史数据训练得到的，数据越多，神经网络就越接近真实。神经网络建立后，就能够通过不同的输入变量值，预测输出结果。例如，银行能够通过历史申请贷款的客户资料，建立一个神经网络模型，用于预测以后申请贷款客户的违约情况，做出是否贷款给该客户的决策。</p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>BP神经网络是一种多层的前馈神经网络，其主要的特点是：</p>
<ul>
<li>信号是前向传播的</li>
<li>误差是反向传播的</li>
</ul>
<p>BP神经网络的过程主要分为两个阶段，第一阶段是信号的前向传播，从输入层经过隐含层，最后到达输出层；第二阶段是误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置。</p>
<p><img src="E:\学习\BP\1.png" alt="alt 1"></p>
<h3 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h3><p>人工神经网络无需事先确定输入输出之间映射关系的数学方程，仅通过自身的训练，学习某种规则，在给定输入值时得到最接近期望输出值的结果。作为一种智能信息处理系统，人工神经网络实现其功能的核心是算法。BP神经网络是一种按误差反向传播(简称误差反传)训练的多层前馈网络，其算法称为BP算法，它的基本思想是梯度下降法，利用梯度搜索技术，以期使网络的实际输出值和期望输出值的误差均方差为最小。</p>
<p>基本BP算法包括信号的前向传播和误差的反向传播两个过程。即计算误差输出时按从输入到输出的方向进行，而调整权值和阈值则从输出到输入的方向进行。正向传播时，输入信号通过隐含层作用于输出节点，经过非线性变换，产生输出信号，若实际输出与期望输出不相符，则转入误差的反向传播过程。误差反传是将输出误差通过隐含层向输入层逐层反传，并将误差分摊给各层所有单元，以从各层获得的误差信号作为调整各单元权值的依据。通过调整输入节点与隐层节点的联接强度和隐层节点与输出节点的联接强度以及阈值，使误差沿梯度方向下降，经过反复学习训练，确定与最小误差相对应的网络参数(权值和阈值)，训练即告停止。此时经过训练的神经网络即能对类似样本的输入信息，自行处理输出误差最小的经过非线形转换的信息。</p>
<h3 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h3><p><img src="E:\学习\BP\2.png" alt="alt 1"></p>
<p>这个模型中，每个神经元都接受来自其它神经元的输入信号，每个信号都通过一个带有权重的连接传递，神经元把这些信号加起来得到一个总输入值，然后将总输入值与神经元的阈值进行对比（模拟阈值电位），然后通过一个“激活函数”处理得到最终的输出（模拟细胞的激活），这个输出又会作为之后神经元的输入一层一层传递下去。</p>
<ul>
<li>一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量的结果。</li>
<li>X1~Xn为输入向量的各个分量</li>
<li>W1~Wn为神经元各个突触的权值</li>
<li>f为传递函数，通常为非线性函数，如 traingd(),tansig(),hardlim()</li>
</ul>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>引入激活函数的目的是在模型中引入非线性。如果没有激活函数，那么无论你的神经网络有多少层，最终都是一个线性映射，单纯的线性映射无法解决线性不可分问题。</p>
<ul>
<li>激活函数最重要的作用是使神经网络收敛，这就要求激活函数必须是非线性函数。</li>
</ul>
<ol>
<li>线性函数</li>
</ol>
<p>$$f\left ( x \right )&#x3D;k\ast x+c$$</p>
<ol start="2">
<li>阈值函数<br>$$f\left ( x \right )&#x3D;\left{\begin{matrix}0 &amp; x&lt; 0 &amp; \1 &amp;  x\geq 0&amp;\end{matrix}\right.$$</li>
<li>S形函数(sigmoid)</li>
</ol>
<p>$$f\left ( x \right )&#x3D;\frac{1}{1+e^{-x}}$$<br>此函数值域为（0，1），当z的值趋近-∞时，a的值趋近于0；当z的值趋近+∞时，a的值趋近于1。</p>
<p><img src="E:\学习\BP\3.jpg" alt="alt 1"></p>
<h2 id="BP-神经网络ATLAB工具箱"><a href="#BP-神经网络ATLAB工具箱" class="headerlink" title="BP 神经网络ATLAB工具箱"></a>BP 神经网络ATLAB工具箱</h2><h3 id="通过神经网络工具箱GUI界面来创建神经网络"><a href="#通过神经网络工具箱GUI界面来创建神经网络" class="headerlink" title="通过神经网络工具箱GUI界面来创建神经网络"></a>通过神经网络工具箱GUI界面来创建神经网络</h3><p>参考<a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1595694119191885931&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1595694119191885931&amp;wfr=spider&amp;for=pc</a></p>
<h3 id="BP神经网络MATLAB相关函数"><a href="#BP神经网络MATLAB相关函数" class="headerlink" title="BP神经网络MATLAB相关函数"></a>BP神经网络MATLAB相关函数</h3><h4 id="BP网络创建函数"><a href="#BP网络创建函数" class="headerlink" title="BP网络创建函数"></a>BP网络创建函数</h4><h5 id="BP网络创建函数中的变量"><a href="#BP网络创建函数中的变量" class="headerlink" title="BP网络创建函数中的变量"></a>BP网络创建函数中的变量</h5><ul>
<li>RP:每组输入元素最大值和最小值组成的R*2矩阵</li>
<li>Si:第i层长度，共N层</li>
<li>TFi：第i层激励函数，默认为：tansig</li>
<li>BTF:网络的训练函数，默认为：trainlm</li>
<li>BLF:权值和阈值的学习算法，默认为：learnglm</li>
<li>PF:网络的性能函数，默认为：mes</li>
</ul>
<h5 id="函数newcf"><a href="#函数newcf" class="headerlink" title="函数newcf"></a>函数newcf</h5><blockquote>
<p>此函数用于创建级联向前BP网络</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net=netcf</span><br><span class="line">net=netcf(PR,[S1 S2…SN],&#123;TF1 TF2…TFN&#125;,BTF，BLF，PF)</span><br></pre></td></tr></table></figure>
<h5 id="函数newff"><a href="#函数newff" class="headerlink" title="函数newff"></a>函数newff</h5><blockquote>
<p>此函数用于创建一个BP网络</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net=netcf</span><br><span class="line">net=netff(PR,[S1 S2…SN],&#123;TF1 TF2…TFN&#125;,BTF，BLF，PF)</span><br></pre></td></tr></table></figure>
<h4 id="神经元激励函数"><a href="#神经元激励函数" class="headerlink" title="神经元激励函数"></a>神经元激励函数</h4><h5 id="函数logsig"><a href="#函数logsig" class="headerlink" title="函数logsig"></a>函数logsig</h5><p>S型对数函数，使用格式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=logsig(N)</span><br><span class="line">info=logsig(code)</span><br></pre></td></tr></table></figure>
<ul>
<li>N为Q个S维的输入列向量</li>
<li>A为函数返回值</li>
<li>位于区间(0,1)</li>
</ul>
<p>根据code的值返回不同信息</p>
<table>
<thead>
<tr>
<th>code取值</th>
<th>返回信息</th>
</tr>
</thead>
<tbody><tr>
<td>deviv</td>
<td>微分函数的名称</td>
</tr>
<tr>
<td>output</td>
<td>输出值域</td>
</tr>
<tr>
<td>name</td>
<td>函数全称</td>
</tr>
<tr>
<td>active</td>
<td>有效的输入区间</td>
</tr>
</tbody></table>
<ul>
<li>实用算法：logsig(n)&#x3D;1&#x2F;(1+exp(-n))</li>
</ul>
<p>图像</p>
<p><img src="E:\学习\BP\4.jpg" alt="alt 1"></p>
<h5 id="函数dlogsig：logsig的导数"><a href="#函数dlogsig：logsig的导数" class="headerlink" title="函数dlogsig：logsig的导数"></a>函数dlogsig：logsig的导数</h5><p>使用格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dA_dN=dlogsig(N,A)</span><br></pre></td></tr></table></figure>
<h5 id="函数tansig"><a href="#函数tansig" class="headerlink" title="函数tansig"></a>函数tansig</h5><p>双曲正切S型激励函数，使用格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=tansig(N)</span><br><span class="line">info=tansig(code)</span><br></pre></td></tr></table></figure>
<ul>
<li>N为Q个S维的输入列向量</li>
<li>A为函数返回值</li>
<li>位于区间(-1,1)</li>
</ul>
<p>根据code的值返回不同信息</p>
<p>算法：<br>$$n&#x3D;\frac{2}{[1+exp(-2n)]-1}$$</p>
<h5 id="函数dtansig：tansig的导数"><a href="#函数dtansig：tansig的导数" class="headerlink" title="函数dtansig：tansig的导数"></a>函数dtansig：tansig的导数</h5><p>使用格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dA_dN=tansig(N,A)</span><br></pre></td></tr></table></figure>
<h5 id="函数purelin"><a href="#函数purelin" class="headerlink" title="函数purelin"></a>函数purelin</h5><p>线性激励函数,用法同上</p>
<h4 id="BP网络学习函数"><a href="#BP网络学习函数" class="headerlink" title="BP网络学习函数"></a>BP网络学习函数</h4><h5 id="训练函数与学习函数的区别"><a href="#训练函数与学习函数的区别" class="headerlink" title="训练函数与学习函数的区别"></a>训练函数与学习函数的区别</h5><p>训练函数和学习函数是两个不同的函数 ，网络设置中两个都有。简单的说，训练函数确定调整的大算法，是全局调整权值和阈值，考虑的是整体误差的最小； 学习函数决定调整量怎么确定，是局部调整权值和阈值，考虑的是单个神经元误差的最小。所以两者不冲突，可以一样也可以不同，就像你绕着楼跑步时，地球也在绕着太阳跑，是局部与整体的区别，既有联系又有区别，辩证统一。</p>
<p>训练函数是如何让误差最小的一些算法，如梯度下降，共轭梯度，这里强调算法。学习函数是指，权值和阈值的调整规则，或者称更新规则。训练函数求得权值或阈值之后，由学习函数进行调整，然后再由训练函数训练新的权值或阈值，然后再调整，反复下去。</p>
<h5 id="函数learngd"><a href="#函数learngd" class="headerlink" title="函数learngd"></a>函数learngd</h5><p>梯度下降权值&#x2F;阈值学习函数，通过神经元的输入与误差，以及权值、阈值的学习速率计算权值和阈值的变化率，使用格式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[dW,LS]=learngd(W,P,Z,N,A,T,E,gW,gA,D,LP,LS)</span><br><span class="line">[db,LS]=learngd(b,ones(1,Q),Z,N,A,T,E,gW,gA,D,LP,LS)</span><br><span class="line">info=learngd(code)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>W为S×R维的权值矩阵；</li>
<li>b为S维的阈值向量；</li>
<li>P为Q组R维的输入向量；</li>
<li>ones(1，Q)为产生一个Q维的输入向量；</li>
<li>Z为Q组S维的加权输入向量；</li>
<li>N为Q组S维的输入向量；</li>
<li>A为Q组S维的输出向量；</li>
<li>T为Q组S维的层目标函数；</li>
<li>E为Q组S维的层误差向量；</li>
<li>gW为与性能相关的S×R维梯度；gA为与性能相关的S×R维输出梯度：D为S×5维的神经元距离矩阵：</li>
<li>LP为学习参数，可通过该参数设置学习速率，设置格式如LP.lr&#x3D;0.001；LS为学习状态，初始状态下为空：</li>
<li>dW为S×R维的权值或值变化率矩阵：为S维的值变化率向量；ls为新的学习状态 learngd(code)依据不同的code值返回有关函数的不用信息，具体有： pnames(返回设置的学习参数)，pdeauls(一返回默认的学习参数)，needg(如果函数使用了gW或者gA，则返回1)</li>
</ul>
<h5 id="函数-learngdm："><a href="#函数-learngdm：" class="headerlink" title="函数 learngdm："></a>函数 learngdm：</h5><p>梯度下降动量学习函数，它利用神经元的输入和误差，权值和值的学习速率和动量常数计算权值或阈值的变化率，</p>
<p>其使用格式为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[dW,LS]=learngdm(W,P,Z,N,A,T,E,gW,gA,D,LP,LS)</span><br><span class="line">[db,LS]=learngdm(b,ones(1,Q),Z,N,A,T,E,gW,gA,D,LP,LS)</span><br><span class="line">info=learngd(code)</span><br></pre></td></tr></table></figure>
<p>以上程序中各参数的含义与函数 learngd相同<br>另外，动量常数mc是通过学习参数LP设置的，格式为LP.mc&#x3D;0.8.</p>
<h4 id="BP网络训练函数"><a href="#BP网络训练函数" class="headerlink" title="BP网络训练函数"></a>BP网络训练函数</h4><h5 id="函数-trainbfg"><a href="#函数-trainbfg" class="headerlink" title="函数 trainbfg"></a>函数 trainbfg</h5><p>BFGS准牛顿BP算法函数，除了BP网络外，该函数也可以训练任意形式的神经网络，要求它的激励函数对于权值和输入存在导数，使用格式为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[net, TR,Ac, El]=trainbfg(NET, Pd,TI,Ai, Q, TS, VV,TV)</span><br><span class="line">info= trainbfg(code)</span><br></pre></td></tr></table></figure>
<p>其中，</p>
<ul>
<li>NET为待训练的神经网络：</li>
<li>Pad为有延迟的输入向量：</li>
<li>T为层次目标向量；</li>
<li>A为初始的输入延迟条件</li>
<li>Q为批量</li>
<li>TS为时间步长；</li>
<li>VV用于确认向量结构或者为空；</li>
<li>TV用于检验向量结构或者为空</li>
<li>net为训练后的神经网络；</li>
<li>TR为每步训练的有关信息记录，包括TR.epoch一时刻点，TR.perf训练性能，TR.vperf-确认性能，TR.tperf一检验性</li>
<li>Ac为上一步训练中聚合层的输出；</li>
<li>E1为上一步训练中的层次误差；</li>
<li>info&#x3D; trainbfg(code)依据不同的code值返<br>回不同的信息，<br>在利用该函数进行BP网络训练时， Matlab已经默认了一些训练参<br>数，具体参考相关文档</li>
</ul>
<h5 id="函数traingd"><a href="#函数traingd" class="headerlink" title="函数traingd"></a>函数traingd</h5><p>同上</p>
<h5 id="函数traingdm"><a href="#函数traingdm" class="headerlink" title="函数traingdm"></a>函数traingdm</h5><p>同上上</p>
<h4 id="性能函数"><a href="#性能函数" class="headerlink" title="性能函数"></a>性能函数</h4><h5 id="函数mse"><a href="#函数mse" class="headerlink" title="函数mse"></a>函数mse</h5><p>均方误差性能函数，使用格式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perf=mes(e,x,pp)</span><br><span class="line">perf=mes(e,net,pp)</span><br><span class="line">info=mes(code)</span><br></pre></td></tr></table></figure>
<p>其中，</p>
<ul>
<li>e为误差向量矩阵</li>
<li>x为所有权值和阈值向量</li>
<li>pp为性能参数</li>
<li>net为待评定的神经网络</li>
<li>perf为函数的返回值，平均绝对误差</li>
<li>mse(code)为根据code值的不同，返回不同的信息</li>
</ul>
<h5 id="函数-msereg"><a href="#函数-msereg" class="headerlink" title="函数 msereg"></a>函数 msereg</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perf=msereg(e. x, pp)</span><br><span class="line">perf=msereg(e. net)</span><br><span class="line">info-msereg(code)</span><br></pre></td></tr></table></figure>
<p>以上程序中各参数的含义与函数mse相同，函数 msereg是在函数mse基础上增加一项网络权值和值的均方值，目的是使网络获得较小的权值和值，从而迫使网络的响应变得更平滑，</p>
<h2 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h2><h3 id="神经元节点数选择"><a href="#神经元节点数选择" class="headerlink" title="神经元节点数选择"></a>神经元节点数选择</h3><p>有关研究表明, 有一个隐层的神经网络, 只要隐节点足够多, 就可以以任意精度逼近一个非线性函数。因此, 本文采用含有一个隐层的三层多输入单输出的BP网络建立预测模型。在网络设计过程中, 隐层神经元数的确定十分重要。隐层神经元个数过多, 会加大网络计算量并容易产生过度拟合问题; 神经元个数过少, 则会影响网络性能, 达不到预期效果。</p>
<p>网络中隐层神经元的数目与实际问题的复杂程度、输入和输出层的神经元数以及对期望误差的设定有着直接的联系。目前, 对于隐层中神经元数目的确定并没有明确的公式, 只有一些经验公式, 神经元个数的最终确定还是需要根据经验和多次实验来确定。</p>
<p>$$l&#x3D;\sqrt{m+n}+a$$</p>
<p>其中, n为输入层神经元个数, m 为输出层神经元个数, a 为[ 1, 10]之间的常数。</p>
<h3 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h3><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/bhlsheji/p/4249492.html">https://www.cnblogs.com/bhlsheji/p/4249492.html</a></p>
</blockquote>
<h4 id="什么是归一化？"><a href="#什么是归一化？" class="headerlink" title="什么是归一化？"></a>什么是归一化？</h4><p>数据归一化，就是将数据映射到[0,1]或[-1,1]区间或更小的区间，比方(0.1,0.9) 。</p>
<h4 id="为什么要归一化处理？"><a href="#为什么要归一化处理？" class="headerlink" title="为什么要归一化处理？"></a>为什么要归一化处理？</h4><ul>
<li>输入数据的单位不一样，有些数据的范围可能特别大，导致的结果是神经网络收敛慢、训练时间长。</li>
<li>数据范围大的输入在模式分类中的作用可能会偏大，而数据范围小的输入作用就可能会偏小。</li>
<li>因为神经网络输出层的激活函数的值域是有限制的，因此须要将网络训练的目标数据映射到激活函数的值域。比如神经网络的输出层若採用S形激活函数，因为S形函数的值域限制在(0,1)，也就是说神经网络的输出仅仅能限制在(0,1)，所以训练数据的输出就要归一化到[0,1]区间。</li>
<li>S形激活函数在(0,1)区间以外区域非常平缓，区分度太小。比如S形函数f(X)在參数a&#x3D;1时，f(100)与f(5)仅仅相差0.0067。</li>
</ul>
<h4 id="归一化算法"><a href="#归一化算法" class="headerlink" title="归一化算法"></a>归一化算法</h4><p>　　一种简单而高速的归一化算法是线性转换算法。线性转换算法常见有两种形式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = ( x - min )/( max - min )</span><br></pre></td></tr></table></figure>
<p>　　当中min为x的最小值，max为x的最大值，输入向量为x，归一化后的输出向量为y 。上式将数据归一化到 [ 0 , 1 ]区间，当激活函数採用S形函数时（值域为(0,1)）时这条式子适用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = 2 * ( x - min ) / ( max - min ) - 1</span><br></pre></td></tr></table></figure>
<p>这条公式将数据归一化到 [ -1 , 1 ] 区间。当激活函数採用双极S形函数（值域为(-1,1)）时这条式子适用。</p>
<h4 id="Matlab数据归一化处理函数"><a href="#Matlab数据归一化处理函数" class="headerlink" title="Matlab数据归一化处理函数"></a>Matlab数据归一化处理函数</h4><p>　　Matlab中归一化处理数据能够采用<code>premnmx</code> ， <code>postmnmx </code>， <code>tramnmx</code> 这3个函数。</p>
<h5 id="premnmx"><a href="#premnmx" class="headerlink" title="premnmx"></a>premnmx</h5><p>语法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[pn,minp,maxp,tn,mint,maxt] = premnmx(p,t)</span><br></pre></td></tr></table></figure>
<p>參数：</p>
<ul>
<li>pn： p矩阵按行归一化后的矩阵</li>
<li>minp，maxp：p矩阵每一行的最小值，最大值</li>
<li>tn：t矩阵按行归一化后的矩阵</li>
<li>mint，maxt：t矩阵每一行的最小值，最大值<br>作用：将矩阵p，t归一化到[-1,1] ，主要用于归一化处理训练数据集。</li>
</ul>
<h5 id="tramnmx"><a href="#tramnmx" class="headerlink" title="tramnmx"></a>tramnmx</h5><p>语法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[pn] = tramnmx(p,minp,maxp)</span><br></pre></td></tr></table></figure>
<p>參数：</p>
<ul>
<li>minp，maxp：premnmx函数计算的矩阵的最小，最大值</li>
<li>pn：归一化后的矩阵<br>作用：主要用于归一化处理待分类的输入数据。</li>
</ul>
<h5 id="postmnmx"><a href="#postmnmx" class="headerlink" title="postmnmx"></a>postmnmx</h5><p>语法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[p,t] = postmnmx(pn,minp,maxp,tn,mint,maxt)</span><br></pre></td></tr></table></figure>
<p>參数：</p>
<ul>
<li>minp，maxp：premnmx函数计算的p矩阵每行的最小值，最大值</li>
<li>mint，maxt：premnmx函数计算的t矩阵每行的最小值，最大值</li>
</ul>
<p>作用：将矩阵pn，tn映射回归一化处理前的范围。postmnmx函数主要用于将神经网络的输出结果映射回归一化前的数据范围。</p>
<h3 id="MATLAB实现案例"><a href="#MATLAB实现案例" class="headerlink" title="MATLAB实现案例"></a>MATLAB实现案例</h3><blockquote>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/wuchangi/article/details/79236016">https://blog.csdn.net/wuchangi/article/details/79236016</a></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">numberOfSample = 20; %输入样本数量</span><br><span class="line">%取测试样本数量等于输入(训练集)样本数量，因为输入样本（训练集）容量较少，否则一般必须用新鲜数据进行测试</span><br><span class="line">numberOfTestSample = 20;</span><br><span class="line">numberOfForcastSample = 2;</span><br><span class="line">numberOfHiddenNeure = 8;</span><br><span class="line">inputDimension = 3;</span><br><span class="line">outputDimension = 2;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%准备好训练集</span><br><span class="line"></span><br><span class="line">%人数(单位：万人)</span><br><span class="line">numberOfPeople=[20.55 22.44 25.37 27.13 29.45 30.10 30.96 34.06 36.42 38.09 39.13 39.99 41.93 44.59 47.30 52.89 55.73 56.76 59.17 60.63];</span><br><span class="line">%机动车数(单位：万辆)</span><br><span class="line">numberOfAutomobile=[0.6 0.75 0.85 0.9 1.05 1.35 1.45 1.6 1.7 1.85 2.15 2.2 2.25 2.35 2.5 2.6 2.7 2.85 2.95 3.1];</span><br><span class="line">%公路面积(单位：万平方公里)</span><br><span class="line">roadArea=[0.09 0.11 0.11 0.14 0.20 0.23 0.23 0.32 0.32 0.34 0.36 0.36 0.38 0.49 0.56 0.59 0.59 0.67 0.69 0.79];</span><br><span class="line">%公路客运量(单位：万人)</span><br><span class="line">passengerVolume = [5126 6217 7730 9145 10460 11387 12353 15750 18304 19836 21024 19490 20433 22598 25107 33442 36836 40548 42927 43462];</span><br><span class="line">%公路货运量(单位：万吨)</span><br><span class="line">freightVolume = [1237 1379 1385 1399 1663 1714 1834 4322 8132 8936 11099 11203 10524 11115 13320 16762 18673 20724 20803 21804];</span><br><span class="line"></span><br><span class="line">%由系统时钟种子产生随机数</span><br><span class="line">rand(&#x27;state&#x27;, sum(100*clock));</span><br><span class="line"></span><br><span class="line">%输入数据矩阵</span><br><span class="line">input = [numberOfPeople; numberOfAutomobile; roadArea];</span><br><span class="line">%目标（输出）数据矩阵</span><br><span class="line">output = [passengerVolume; freightVolume];</span><br><span class="line"></span><br><span class="line">%对训练集中的输入数据矩阵和目标数据矩阵进行归一化处理</span><br><span class="line">[sampleInput, minp, maxp, tmp, mint, maxt] = premnmx(input, output);</span><br><span class="line"></span><br><span class="line">%噪声强度</span><br><span class="line">noiseIntensity = 0.01;</span><br><span class="line">%利用正态分布产生噪声</span><br><span class="line">noise = noiseIntensity * randn(outputDimension, numberOfSample);</span><br><span class="line">%给样本输出矩阵tmp添加噪声，防止网络过度拟合</span><br><span class="line">sampleOutput = tmp + noise;</span><br><span class="line"></span><br><span class="line">%取测试样本输入(输出)与输入样本相同，因为输入样本（训练集）容量较少，否则一般必须用新鲜数据进行测试</span><br><span class="line">testSampleInput = sampleInput;</span><br><span class="line">testSampleOutput = sampleOutput;</span><br><span class="line"></span><br><span class="line">%最大训练次数</span><br><span class="line">maxEpochs = 50000;</span><br><span class="line"></span><br><span class="line">%网络的学习速率</span><br><span class="line">learningRate = 0.035;</span><br><span class="line"></span><br><span class="line">%训练网络所要达到的目标误差</span><br><span class="line">error0 = 0.65*10^(-3);</span><br><span class="line"></span><br><span class="line">%初始化输入层与隐含层之间的权值</span><br><span class="line">W1 = 0.5 * rand(numberOfHiddenNeure, inputDimension) - 0.1;</span><br><span class="line">%初始化输入层与隐含层之间的阈值</span><br><span class="line">B1 = 0.5 * rand(numberOfHiddenNeure, 1) - 0.1;</span><br><span class="line">%初始化输出层与隐含层之间的权值</span><br><span class="line">W2 = 0.5 * rand(outputDimension, numberOfHiddenNeure) - 0.1;</span><br><span class="line">%初始化输出层与隐含层之间的阈值</span><br><span class="line">B2 = 0.5 * rand(outputDimension, 1) - 0.1;</span><br><span class="line"></span><br><span class="line">%保存能量函数(误差平方和)的历史记录</span><br><span class="line">errorHistory = [];</span><br><span class="line"></span><br><span class="line">for i = 1:maxEpochs</span><br><span class="line">    %隐含层输出</span><br><span class="line">    hiddenOutput = logsig(W1 * sampleInput + repmat(B1, 1, numberOfSample));</span><br><span class="line">    %输出层输出</span><br><span class="line">    networkOutput = W2 * hiddenOutput + repmat(B2, 1, numberOfSample);</span><br><span class="line">    %实际输出与网络输出之差</span><br><span class="line">    error = sampleOutput - networkOutput;</span><br><span class="line">    %计算能量函数(误差平方和)</span><br><span class="line">    E = sumsqr(error);</span><br><span class="line">    errorHistory = [errorHistory E];</span><br><span class="line"></span><br><span class="line">    if E &lt; error0</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    %以下依据能量函数的负梯度下降原理对权值和阈值进行调整</span><br><span class="line">    delta2 = error;</span><br><span class="line">    delta1 = W2&#x27; * delta2.*hiddenOutput.*(1 - hiddenOutput);</span><br><span class="line"></span><br><span class="line">    dW2 = delta2 * hiddenOutput&#x27;;</span><br><span class="line">    dB2 = delta2 * ones(numberOfSample, 1);</span><br><span class="line"></span><br><span class="line">    dW1 = delta1 * sampleInput&#x27;;</span><br><span class="line">    dB1 = delta1 * ones(numberOfSample, 1);</span><br><span class="line"></span><br><span class="line">    W2 = W2 + learningRate * dW2;</span><br><span class="line">    B2 = B2 + learningRate * dB2;</span><br><span class="line"></span><br><span class="line">    W1 = W1 + learningRate * dW1;</span><br><span class="line">    B1 = B1 + learningRate * dB1;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%下面对已经训练好的网络进行(仿真)测试</span><br><span class="line"></span><br><span class="line">%对测试样本进行处理</span><br><span class="line">testHiddenOutput = logsig(W1 * testSampleInput + repmat(B1, 1, numberOfTestSample));</span><br><span class="line">testNetworkOutput =  W2 * testHiddenOutput + repmat(B2, 1, numberOfTestSample);</span><br><span class="line">%还原网络输出层的结果(反归一化)</span><br><span class="line">a = postmnmx(testNetworkOutput, mint, maxt);</span><br><span class="line"></span><br><span class="line">%绘制测试样本神经网络输出和实际样本输出的对比图(figure(1))--------------------------------------</span><br><span class="line">t = 1990:2009;</span><br><span class="line"></span><br><span class="line">%测试样本网络输出客运量</span><br><span class="line">a1 = a(1,:);</span><br><span class="line">%测试样本网络输出货运量</span><br><span class="line">a2 = a(2,:);</span><br><span class="line"></span><br><span class="line">figure(1);</span><br><span class="line">subplot(2, 1, 1); plot(t, a1, &#x27;ro&#x27;, t, passengerVolume, &#x27;b+&#x27;);</span><br><span class="line">legend(&#x27;网络输出客运量&#x27;, &#x27;实际客运量&#x27;);</span><br><span class="line">xlabel(&#x27;年份&#x27;); ylabel(&#x27;客运量/万人&#x27;);</span><br><span class="line">title(&#x27;神经网络客运量学习与测试对比图&#x27;);</span><br><span class="line">grid on;</span><br><span class="line"></span><br><span class="line">subplot(2, 1, 2); plot(t, a2, &#x27;ro&#x27;, t, freightVolume, &#x27;b+&#x27;);</span><br><span class="line">legend(&#x27;网络输出货运量&#x27;, &#x27;实际货运量&#x27;);</span><br><span class="line">xlabel(&#x27;年份&#x27;); ylabel(&#x27;货运量/万吨&#x27;);</span><br><span class="line">title(&#x27;神经网络货运量学习与测试对比图&#x27;);</span><br><span class="line">grid on;</span><br><span class="line"></span><br><span class="line">%使用训练好的神经网络对新输入数据进行预测</span><br><span class="line"></span><br><span class="line">%新输入数据(2010年和2011年的相关数据)</span><br><span class="line">newInput = [73.39 75.55; 3.9635 4.0975; 0.9880 1.0268];</span><br><span class="line"></span><br><span class="line">%利用原始输入数据(训练集的输入数据)的归一化参数对新输入数据进行归一化</span><br><span class="line">newInput = tramnmx(newInput, minp, maxp);</span><br><span class="line"></span><br><span class="line">newHiddenOutput = logsig(W1 * newInput + repmat(B1, 1, numberOfForcastSample));</span><br><span class="line">newOutput = W2 * newHiddenOutput + repmat(B2, 1, numberOfForcastSample);</span><br><span class="line">newOutput = postmnmx(newOutput, mint, maxt);</span><br><span class="line"></span><br><span class="line">disp(&#x27;预测2010和2011年的公路客运量分别为(单位：万人)：&#x27;);</span><br><span class="line">newOutput(1,:)</span><br><span class="line">disp(&#x27;预测2010和2011年的公路货运量分别为(单位：万吨)：&#x27;);</span><br><span class="line">newOutput(2,:)</span><br><span class="line"></span><br><span class="line">%在figure(1)的基础上绘制2010和2011年的预测情况-------------------------------------------------</span><br><span class="line">figure(2);</span><br><span class="line">t1 = 1990:2011;</span><br><span class="line"></span><br><span class="line">subplot(2, 1, 1); plot(t1, [a1 newOutput(1,:)], &#x27;ro&#x27;, t, passengerVolume, &#x27;b+&#x27;);</span><br><span class="line">legend(&#x27;网络输出客运量&#x27;, &#x27;实际客运量&#x27;);</span><br><span class="line">xlabel(&#x27;年份&#x27;); ylabel(&#x27;客运量/万人&#x27;);</span><br><span class="line">title(&#x27;神经网络客运量学习与测试对比图(添加了预测数据)&#x27;);</span><br><span class="line">grid on;</span><br><span class="line"></span><br><span class="line">subplot(2, 1, 2); plot(t1, [a2 newOutput(2,:)], &#x27;ro&#x27;, t, freightVolume, &#x27;b+&#x27;);</span><br><span class="line">legend(&#x27;网络输出货运量&#x27;, &#x27;实际货运量&#x27;);</span><br><span class="line">xlabel(&#x27;年份&#x27;); ylabel(&#x27;货运量/万吨&#x27;);</span><br><span class="line">title(&#x27;神经网络货运量学习与测试对比图(添加了预测数据)&#x27;);</span><br><span class="line">grid on;</span><br><span class="line"></span><br><span class="line">%观察能量函数(误差平方和)在训练神经网络过程中的变化情况------------------------------------------</span><br><span class="line">figure(3);</span><br><span class="line"></span><br><span class="line">n = length(errorHistory);</span><br><span class="line">t3 = 1:n;</span><br><span class="line">plot(t3, errorHistory, &#x27;r-&#x27;);</span><br><span class="line"></span><br><span class="line">%为了更加清楚地观察出能量函数值的变化情况，这里我只绘制前100次的训练情况</span><br><span class="line">xlim([1 100]);</span><br><span class="line">xlabel(&#x27;训练过程&#x27;);</span><br><span class="line">ylabel(&#x27;能量函数值&#x27;);</span><br><span class="line">title(&#x27;能量函数(误差平方和)在训练神经网络过程中的变化图&#x27;);</span><br><span class="line">grid on;</span><br></pre></td></tr></table></figure>
<p>numberOfSample &#x3D; 20; %输入样本数量<br>%取测试样本数量等于输入(训练集)样本数量，因为输入样本（训练集）容量较少，否则一般必须用新鲜数据进行测试<br>numberOfTestSample &#x3D; 20;<br>numberOfForcastSample &#x3D; 2;<br>numberOfHiddenNeure &#x3D; 8;<br>inputDimension &#x3D; 3;<br>outputDimension &#x3D; 2;</p>
<p>%准备好训练集</p>
<p>%人数(单位：万人)<br>numberOfPeople&#x3D;[20.55 22.44 25.37 27.13 29.45 30.10 30.96 34.06 36.42 38.09 39.13 39.99 41.93 44.59 47.30 52.89 55.73 56.76 59.17 60.63];<br>%机动车数(单位：万辆)<br>numberOfAutomobile&#x3D;[0.6 0.75 0.85 0.9 1.05 1.35 1.45 1.6 1.7 1.85 2.15 2.2 2.25 2.35 2.5 2.6 2.7 2.85 2.95 3.1];<br>%公路面积(单位：万平方公里)<br>roadArea&#x3D;[0.09 0.11 0.11 0.14 0.20 0.23 0.23 0.32 0.32 0.34 0.36 0.36 0.38 0.49 0.56 0.59 0.59 0.67 0.69 0.79];<br>%公路客运量(单位：万人)<br>passengerVolume &#x3D; [5126 6217 7730 9145 10460 11387 12353 15750 18304 19836 21024 19490 20433 22598 25107 33442 36836 40548 42927 43462];<br>%公路货运量(单位：万吨)<br>freightVolume &#x3D; [1237 1379 1385 1399 1663 1714 1834 4322 8132 8936 11099 11203 10524 11115 13320 16762 18673 20724 20803 21804];</p>
<p>%由系统时钟种子产生随机数<br>rand(‘state’, sum(100*clock));</p>
<p>%输入数据矩阵<br>input &#x3D; [numberOfPeople; numberOfAutomobile; roadArea];<br>%目标（输出）数据矩阵<br>output &#x3D; [passengerVolume; freightVolume];</p>
<p>%对训练集中的输入数据矩阵和目标数据矩阵进行归一化处理<br>[sampleInput, minp, maxp, tmp, mint, maxt] &#x3D; premnmx(input, output);</p>
<p>%噪声强度<br>noiseIntensity &#x3D; 0.01;<br>%利用正态分布产生噪声<br>noise &#x3D; noiseIntensity * randn(outputDimension, numberOfSample);<br>%给样本输出矩阵tmp添加噪声，防止网络过度拟合<br>sampleOutput &#x3D; tmp + noise;</p>
<p>%取测试样本输入(输出)与输入样本相同，因为输入样本（训练集）容量较少，否则一般必须用新鲜数据进行测试<br>testSampleInput &#x3D; sampleInput;<br>testSampleOutput &#x3D; sampleOutput;</p>
<p>%最大训练次数<br>maxEpochs &#x3D; 50000;</p>
<p>%网络的学习速率<br>learningRate &#x3D; 0.035;</p>
<p>%训练网络所要达到的目标误差<br>error0 &#x3D; 0.65*10^(-3);</p>
<p>%初始化输入层与隐含层之间的权值<br>W1 &#x3D; 0.5 * rand(numberOfHiddenNeure, inputDimension) - 0.1;<br>%初始化输入层与隐含层之间的阈值<br>B1 &#x3D; 0.5 * rand(numberOfHiddenNeure, 1) - 0.1;<br>%初始化输出层与隐含层之间的权值<br>W2 &#x3D; 0.5 * rand(outputDimension, numberOfHiddenNeure) - 0.1;<br>%初始化输出层与隐含层之间的阈值<br>B2 &#x3D; 0.5 * rand(outputDimension, 1) - 0.1;</p>
<p>%保存能量函数(误差平方和)的历史记录<br>errorHistory &#x3D; [];</p>
<p>for i &#x3D; 1:maxEpochs<br>    %隐含层输出<br>    hiddenOutput &#x3D; logsig(W1 * sampleInput + repmat(B1, 1, numberOfSample));<br>    %输出层输出<br>    networkOutput &#x3D; W2 * hiddenOutput + repmat(B2, 1, numberOfSample);<br>    %实际输出与网络输出之差<br>    error &#x3D; sampleOutput - networkOutput;<br>    %计算能量函数(误差平方和)<br>    E &#x3D; sumsqr(error);<br>    errorHistory &#x3D; [errorHistory E];</p>
<pre><code>if E &lt; error0
    break;
end

%以下依据能量函数的负梯度下降原理对权值和阈值进行调整
delta2 = error;
delta1 = W2&#39; * delta2.*hiddenOutput.*(1 - hiddenOutput);

dW2 = delta2 * hiddenOutput&#39;;
dB2 = delta2 * ones(numberOfSample, 1);

dW1 = delta1 * sampleInput&#39;;
dB1 = delta1 * ones(numberOfSample, 1);

W2 = W2 + learningRate * dW2;
B2 = B2 + learningRate * dB2;

W1 = W1 + learningRate * dW1;
B1 = B1 + learningRate * dB1;
</code></pre>
<p>end</p>
<p>%下面对已经训练好的网络进行(仿真)测试</p>
<p>%对测试样本进行处理<br>testHiddenOutput &#x3D; logsig(W1 * testSampleInput + repmat(B1, 1, numberOfTestSample));<br>testNetworkOutput &#x3D;  W2 * testHiddenOutput + repmat(B2, 1, numberOfTestSample);<br>%还原网络输出层的结果(反归一化)<br>a &#x3D; postmnmx(testNetworkOutput, mint, maxt);</p>
<p>%绘制测试样本神经网络输出和实际样本输出的对比图(figure(1))————————————–<br>t &#x3D; 1990:2009;</p>
<p>%测试样本网络输出客运量<br>a1 &#x3D; a(1,:);<br>%测试样本网络输出货运量<br>a2 &#x3D; a(2,:);</p>
<p>figure(1);<br>subplot(2, 1, 1); plot(t, a1, ‘ro’, t, passengerVolume, ‘b+’);<br>legend(‘网络输出客运量’, ‘实际客运量’);<br>xlabel(‘年份’); ylabel(‘客运量&#x2F;万人’);<br>title(‘神经网络客运量学习与测试对比图’);<br>grid on;</p>
<p>subplot(2, 1, 2); plot(t, a2, ‘ro’, t, freightVolume, ‘b+’);<br>legend(‘网络输出货运量’, ‘实际货运量’);<br>xlabel(‘年份’); ylabel(‘货运量&#x2F;万吨’);<br>title(‘神经网络货运量学习与测试对比图’);<br>grid on;</p>
<p>%使用训练好的神经网络对新输入数据进行预测</p>
<p>%新输入数据(2010年和2011年的相关数据)<br>newInput &#x3D; [73.39 75.55; 3.9635 4.0975; 0.9880 1.0268];</p>
<p>%利用原始输入数据(训练集的输入数据)的归一化参数对新输入数据进行归一化<br>newInput &#x3D; tramnmx(newInput, minp, maxp);</p>
<p>newHiddenOutput &#x3D; logsig(W1 * newInput + repmat(B1, 1, numberOfForcastSample));<br>newOutput &#x3D; W2 * newHiddenOutput + repmat(B2, 1, numberOfForcastSample);<br>newOutput &#x3D; postmnmx(newOutput, mint, maxt);</p>
<p>disp(‘预测2010和2011年的公路客运量分别为(单位：万人)：’);<br>newOutput(1,:)<br>disp(‘预测2010和2011年的公路货运量分别为(单位：万吨)：’);<br>newOutput(2,:)</p>
<p>%在figure(1)的基础上绘制2010和2011年的预测情况————————————————-<br>figure(2);<br>t1 &#x3D; 1990:2011;</p>
<p>subplot(2, 1, 1); plot(t1, [a1 newOutput(1,:)], ‘ro’, t, passengerVolume, ‘b+’);<br>legend(‘网络输出客运量’, ‘实际客运量’);<br>xlabel(‘年份’); ylabel(‘客运量&#x2F;万人’);<br>title(‘神经网络客运量学习与测试对比图(添加了预测数据)’);<br>grid on;</p>
<p>subplot(2, 1, 2); plot(t1, [a2 newOutput(2,:)], ‘ro’, t, freightVolume, ‘b+’);<br>legend(‘网络输出货运量’, ‘实际货运量’);<br>xlabel(‘年份’); ylabel(‘货运量&#x2F;万吨’);<br>title(‘神经网络货运量学习与测试对比图(添加了预测数据)’);<br>grid on;</p>
<p>%观察能量函数(误差平方和)在训练神经网络过程中的变化情况——————————————<br>figure(3);</p>
<p>n &#x3D; length(errorHistory);<br>t3 &#x3D; 1:n;<br>plot(t3, errorHistory, ‘r-‘);</p>
<p>%为了更加清楚地观察出能量函数值的变化情况，这里我只绘制前100次的训练情况<br>xlim([1 100]);<br>xlabel(‘训练过程’);<br>ylabel(‘能量函数值’);<br>title(‘能量函数(误差平方和)在训练神经网络过程中的变化图’);<br>grid on;</p>
<pre><code>
</code></pre>

        </div>
        
          


  <section class='meta' id="footer-meta">
    <hr>
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2022-10-28T20:43:05+08:00">
  <a class='notlink'>
    <i class="fas fa-clock" aria-hidden="true"></i>
    <p>last updated at Oct 28, 2022</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/" rel="nofollow"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;<p>智能算法</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="QQ好友" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://example.com/2019/08/24/20190824shenjing/&title=BP神经网络 | BIEBLOG&summary=神经网络算法能够通过大量的历史数据，逐步建立和完善输入变量到输出结果之间的发展路径，也就是神经网络，在这个神经网络中，每条神经的建立以及神经的粗细（权重）都是经过大量历史数据训练得到的，数据越多，神经网络就越接近真实。神经网络建立后，就能够通过不同的输入变量值，预测输出结果"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="QQ空间" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://example.com/2019/08/24/20190824shenjing/&title=BP神经网络 | BIEBLOG&summary=神经网络算法能够通过大量的历史数据，逐步建立和完善输入变量到输出结果之间的发展路径，也就是神经网络，在这个神经网络中，每条神经的建立以及神经的粗细（权重）都是经过大量历史数据训练得到的，数据越多，神经网络就越接近真实。神经网络建立后，就能够通过不同的输入变量值，预测输出结果"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="微博" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://service.weibo.com/share/share.php?url=http://example.com/2019/08/24/20190824shenjing/&title=BP神经网络 | BIEBLOG&summary=神经网络算法能够通过大量的历史数据，逐步建立和完善输入变量到输出结果之间的发展路径，也就是神经网络，在这个神经网络中，每条神经的建立以及神经的粗细（权重）都是经过大量历史数据训练得到的，数据越多，神经网络就越接近真实。神经网络建立后，就能够通过不同的输入变量值，预测输出结果"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
            <div class="prev-next">
                
                    <section class="prev">
                        <span class="art-item-left">
                            <h6><i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;Previous</h6>
                            <h4>
                                <a href="/2019/09/04/20190904tongji/" rel="prev" title="数学建模统计分析">
                                  
                                      数学建模统计分析
                                  
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/%E7%BB%9F%E8%AE%A1/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>统计</a>
                                </h6>
                            
                        </span>
                    </section>
                
                
                    <section class="next">
                        <span class="art-item-right" aria-hidden="true">
                            <h6>Next&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i></h6>
                            <h4>
                                <a href="/2019/08/24/20190824yichuang/" rel="prev" title="遗传算法">
                                    
                                        遗传算法
                                    
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>智能算法</a>
                                </h6>
                            
                        </span>
                    </section>
                
            </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script>
    window.subData = {
      title: 'BP神经网络',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
    
    
      
        
          
          
            <section class='widget author'>
  <div class='content pure'>
    
      <div class='avatar'>
        <img class='avatar' src='/image/touxiang.jpg'/>
      </div>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="/atom.xml"
              class="social fas fa-rss flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="mailto:me@xaoxuu.com"
              class="social fas fa-envelope flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/BIE5462"
              class="social fab fa-github flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=627711889"
              class="social fas fa-headphones-alt flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;TOC</div>
  
    <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div>
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E6%9C%88%E7%AC%AC%E4%B8%89%E6%AC%A1%E6%B1%87%E6%8A%A5"><span class="toc-text">八月第三次汇报</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">BP神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-text">概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-text">数学原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">神经元模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CATLAB%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="toc-text">BP 神经网络ATLAB工具箱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1GUI%E7%95%8C%E9%9D%A2%E6%9D%A5%E5%88%9B%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">通过神经网络工具箱GUI界面来创建神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CMATLAB%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0"><span class="toc-text">BP神经网络MATLAB相关函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BP%E7%BD%91%E7%BB%9C%E5%88%9B%E5%BB%BA%E5%87%BD%E6%95%B0"><span class="toc-text">BP网络创建函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#BP%E7%BD%91%E7%BB%9C%E5%88%9B%E5%BB%BA%E5%87%BD%E6%95%B0%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F"><span class="toc-text">BP网络创建函数中的变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0newcf"><span class="toc-text">函数newcf</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0newff"><span class="toc-text">函数newff</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="toc-text">神经元激励函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0logsig"><span class="toc-text">函数logsig</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0dlogsig%EF%BC%9Alogsig%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">函数dlogsig：logsig的导数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0tansig"><span class="toc-text">函数tansig</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0dtansig%EF%BC%9Atansig%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">函数dtansig：tansig的导数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0purelin"><span class="toc-text">函数purelin</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BP%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E5%87%BD%E6%95%B0"><span class="toc-text">BP网络学习函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0%E4%B8%8E%E5%AD%A6%E4%B9%A0%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">训练函数与学习函数的区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0learngd"><span class="toc-text">函数learngd</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0-learngdm%EF%BC%9A"><span class="toc-text">函数 learngdm：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BP%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-text">BP网络训练函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0-trainbfg"><span class="toc-text">函数 trainbfg</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0traingd"><span class="toc-text">函数traingd</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0traingdm"><span class="toc-text">函数traingdm</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%87%BD%E6%95%B0"><span class="toc-text">性能函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0mse"><span class="toc-text">函数mse</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0-msereg"><span class="toc-text">函数 msereg</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">模型实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E8%8A%82%E7%82%B9%E6%95%B0%E9%80%89%E6%8B%A9"><span class="toc-text">神经元节点数选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">数据归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%9F"><span class="toc-text">什么是归一化？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-text">为什么要归一化处理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text">归一化算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matlab%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="toc-text">Matlab数据归一化处理函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#premnmx"><span class="toc-text">premnmx</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tramnmx"><span class="toc-text">tramnmx</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#postmnmx"><span class="toc-text">postmnmx</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MATLAB%E5%AE%9E%E7%8E%B0%E6%A1%88%E4%BE%8B"><span class="toc-text">MATLAB实现案例</span></a></li></ol></li></ol></li></ol>
    </div>
  </section>


          
        
      
        
          
          
            
  <section class='widget category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章分类</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/blog/categories/"
    title="blog/categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/LeetCode/" href="/categories/LeetCode/"><div class='name'>LeetCode</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box child" title="/categories/LeetCode/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/" href="/categories/LeetCode/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"><div class='name'>贪心算法</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/matlab/" href="/categories/matlab/"><div class='name'>matlab</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/"><div class='name'>专业知识</div><div class='badge'>(5)</div></a></li>
        
          <li><a class="flat-box child" title="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"><div class='name'>数值分析</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E5%85%B6%E4%BB%96/" href="/categories/%E5%85%B6%E4%BB%96/"><div class='name'>其他</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"><div class='name'>技术笔记</div><div class='badge'>(2)</div></a></li>
        
      </ul>
    </div>
  </section>


          
        
      
        
          
          
            
  <section class='widget tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章标签</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/blog/tags/"
    title="blog/tags/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <a href="/tags/blog/" style="font-size: 24px; color: #555">blog</a> <a href="/tags/matlab/" style="font-size: 14px; color: #999">matlab</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 14px; color: #999">专业知识</a> <a href="/tags/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/" style="font-size: 24px; color: #555">智能算法</a> <a href="/tags/%E7%BB%9F%E8%AE%A1/" style="font-size: 14px; color: #999">统计</a> <a href="/tags/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/" style="font-size: 14px; color: #999">贪心算法</a>
    </div>
  </section>


          
        
      
        
          
          
            


  <section class='widget music'>
    
<header class='pure'>
  <div><i class="fas fa-compact-disc fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;最近在听</div>
  
    <a class="rightBtn"
    
      rel="external nofollow noopener noreferrer"
    
    
      target="_blank"
    
    href="https://music.163.com/#/playlist?id=2962986911"
    title="https://music.163.com/#/playlist?id=2962986911">
    <i class="far fa-heart fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css">
  <div class="aplayer"
    data-theme="#1BCDFC"
    
    
    data-mode="random"
    data-server="netease"
    data-type="playlist"
    data-id="2962986911"
    data-volume="0.5">
  </div>
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script>


    </div>
  </section>


          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  
  
    <div class="social-wrapper">
      
        
          <a href="/atom.xml"
            class="social fas fa-rss flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="mailto:me@xaoxuu.com"
            class="social fas fa-envelope flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/BIE5462"
            class="social fab fa-github flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=627711889"
            class="social fas fa-headphones-alt flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>Blog content follows the <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
  <div>
    Use
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">BIEBLOG</a>
    as theme
    
      , 
      total visits
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      times
    
    . 
  </div>
</footer>
<script>setLoadingBarProgress(80);</script>


      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>


  <!-- fastclick -->
  <script src="https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      FastClick.attach(document.body)
    }, false)
  </script>



  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
    <script type="text/javascript">
      $(function(){
        if ('.cover') {
          $('.cover').backstretch(
          ["/image/beijin01.jpg", "/image/beijin02.jpg", "/image/beijin03.jpg"],
          {
            duration: "7000",
            fade: "6500"
          });
        } else {
          $.backstretch(
          ["/image/beijin01.jpg", "/image/beijin02.jpg", "/image/beijin03.jpg"],
          {
            duration: "7000",
            fade: "6500"
          });
        }
      });
    </script>
  











  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/app.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/search.js"></script>





<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "Copied";
  let COPY_FAILURE = "Copy failed";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>Copy</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
